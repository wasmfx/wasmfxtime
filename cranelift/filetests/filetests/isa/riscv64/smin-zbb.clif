test compile precise-output
set unwind_info=false
target riscv64 has_zbb

function %smin_i8(i8, i8) -> i8{
block0(v0: i8, v1: i8):
    v2 = smin v0, v1
    return v2
}

; VCode:
; block0:
;   sext.b a3,a0
;   sext.b a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x93, 0x16, 0x45, 0x60
;   .byte 0x93, 0x97, 0x45, 0x60
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smin_i16(i16, i16) -> i16{
block0(v0: i16, v1: i16):
    v2 = smin v0, v1
    return v2
}

; VCode:
; block0:
;   sext.h a3,a0
;   sext.h a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x93, 0x16, 0x55, 0x60
;   .byte 0x93, 0x97, 0x55, 0x60
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smin_i32(i32, i32) -> i32{
block0(v0: i32, v1: i32):
    v2 = smin v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w a3,a0
;   sext.w a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sext.w a3, a0
;   sext.w a5, a1
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smin_i64(i64, i64) -> i64{
block0(v0: i64, v1: i64):
    v2 = smin v0, v1
    return v2
}

; VCode:
; block0:
;   max a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x33, 0x65, 0xb5, 0x0a
;   ret

function %smin_i128(i128, i128) -> i128{
block0(v0: i128, v1: i128):
    v2 = smin v0, v1
    return v2
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s3,-8(sp)
;   add sp,-16
; block0:
;   slt a5,[a0,a1],[a2,a3]##ty=i128
;   mv a4,a0
;   mv s3,a1
;   select [a0,a1],[a4,s3],[a2,a3]##condition=(a5 ne zero)
;   add sp,+16
;   ld s3,-8(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s3, -8(sp)
;   addi sp, sp, -0x10
; block1: ; offset 0x18
;   blt a1, a3, 0xc
;   bne a1, a3, 0x10
;   bgeu a0, a2, 0xc
;   addi a5, zero, 1
;   j 8
;   mv a5, zero
;   mv a4, a0
;   mv s3, a1
;   beqz a5, 0x10
;   mv a0, a4
;   mv a1, s3
;   j 0xc
;   mv a0, a2
;   mv a1, a3
;   addi sp, sp, 0x10
;   ld s3, -8(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

